# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: vllm-volume
# spec:
#   accessModes:
#   - ReadWriteMany
#   resources:
#     requests:
#       storage: 120Gi
#   storageClassName: longhorn
#   volumeMode: Filesystem
# ---
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: vllm
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        nodeSelector:
          nvidia.com/gpu.product: NVIDIA-GeForce-RTX-4070
        containers:
          - name: vllm-leader
            image: docker.io/vllm/vllm-openai:latest
            env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-secret
                    key: hf_api_token
                    optional: true
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE); 
                python3 -m pip install bitsandbytes
                python3 -m vllm.entrypoints.openai.api_server --port 8080 --model 'Mungert/Qwen2.5-VL-32B-Instruct-GGUF' --tensor-parallel-size 1 --pipeline_parallel_size 2" # Qwen2.5-VL-32B-Instruct-q3_k_s.gguf
            resources:
              limits:
                nvidia.com/gpu: "1"
                memory: 32Gi
                cpu: "10"
                ephemeral-storage: 150Gi
              requests:
                nvidia.com/gpu: "1"
                memory: 8Gi
                cpu: "2"
                ephemeral-storage: 80Gi
            ports:
              - containerPort: 8080
                protocol: TCP
            readinessProbe:
              tcpSocket:
                port: 8080
              initialDelaySeconds: 15
              periodSeconds: 10
            volumeMounts:
              # - mountPath: /root/.cache/huggingface
              #   name: cache-volume
              - mountPath: /dev/shm
                name: dshm
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
        # - name: cache-volume
        #   persistentVolumeClaim:
        #     claimName: vllm-volume
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            image: docker.io/vllm/vllm-openai:latest
            command:
              - sh
              - -c
              - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);"
            resources:
              limits:
                nvidia.com/gpu: "1"
                memory: 32Gi
                cpu: "10"
                ephemeral-storage: 150Gi
              requests:
                nvidia.com/gpu: "1"
                memory: 8Gi
                cpu: "2"
                ephemeral-storage: 80Gi
            env:
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-secret
                    key: hf_api_token
                    optional: true
            volumeMounts:
              # - mountPath: /root/.cache/huggingface
              #   name: cache-volume
              - mountPath: /dev/shm
                name: dshm   
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
        # - name: cache-volume
        #   persistentVolumeClaim:
        #     claimName: vllm-volume
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-leader
  annotations:
    metallb.universe.tf/loadBalancerIPs: 192.168.0.154
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    leaderworkerset.sigs.k8s.io/name: vllm
    role: leader
  type: ClusterIP
